Non-analogues in paleoecological reconstruction - the effects of analogue distance
========================================================

**Abstract**
*Fossil pollen provides a widespread proxy for past vegetation.  It is increasingly used in paleoclimatic reconstruction, but the limits of its utility are not well known.  Methodological advances, such as the development of tests for spatial autocorrelation, and newer methods for climate reconstruction using boosted regression trees may improve the abilities of these climate reconstruction techniques, but little is known about the accuracy of these models under conditions of non-analogue vegetation known to occur at the end of the late glacial.  In this paper we generate non-analogue pollen assemblages by excluding close neighbours from calibration datasets at increasingly larger squared chord distances.  Using three standard and two newer techniques we then test the ability of pollen-based climate models to accurately recostruct climate under conditions of no-analogue vegetation.*


```{r LoadData, echo=FALSE, warning=FALSE, message=FALSE}

source('R/data_setup.R')

```

Introduction
=========================
Pollen assemblages are widely used for paleoclimatic reconstruction during the Holocene, but this method is also applied to pollen assemblages from the last Glacial period (bartlein2011pollen, kaufman2012multi), and earlier (tarasov2011progress, brewer2008climate, cheddadi1998climate).  A particular challenge for climate reconstruction using these much earlier samples is the presence of non-analogue vegetation.  Non-analogue pollen assemblages pose a problem for many reconstruction techniques, these vegetation assemblages may be the result of sets of climate variables that no longer co-exist, ecological conditions resulting from differing rates of species migration () or release from herbivores (gill2009pleistocene), or as a result of changes in human land use (Goring et al. in review?).  

Predicting climate from pollen in non-analogue space is likely to increase the uncertainty of models, possibly introduce systematic bias in predictions, and is likely to increase the variability of predictions across a time-series of predictions from a single site. However, high-quality terrestrial proxies for climate are important for understanding past climate change, and can act as a constraint on models of climate change in the past derived from GCMs in regions where little proxy data is available.  Thus, understanding the behaviour of pollen-based climate reconstructions in non-analogue space is an important first stem in understanding these reconstructions.

Although we cannot know the values for temperature or precipitation during this time, statistical non-analogues do exist in a modern context, and we can construct false non-analogues using a modified h-block sampling procedure (e.g., Telford & Birks) that examines analogue distance rather than geographical distance.

Methods:
=========================

Sites from across North America were used to build a modern analogue dataset.  Samples were obtained from the North American Modern Pollen Database east of 100oW.  Pollen taxa were standardized as in Gill et al. (in prep), using a set of 54 taxa, similar to those reported in Whitmore et al. (2005).  We chose to exclude Western North America since pollen taxa such as Tsuga canadensis and Tsuga heterophylla may be morphologically indistinct, but taxa have clearly differentiated distributions, thus it may be possible to find false close-analogues using the entire North American dataset.  While this is not, strictly speaking, a major problem for this analysis, it will raise baseline error rates for models which then cause us to underestimate the change in error as analogue distance is increased.

```{r Fig1SiteLocations, fig.width=7, fig.height=6, fig.cap='Figure: North American site locations', echo=FALSE, message=FALSE, warning=FALSE}

map <- map_data('world')

map <- subset(map, map$long > -160 & map$long < -30)
map <- subset(map, map$lat > 20 & map$lat < 83)

ggplot(data = data.frame(map), aes(long, lat)) + 
  geom_polygon(aes(group=group), color='blue', alpha=0.7) +
  geom_point(data = data.frame(climate), aes(x = LONDD, y = LATDD), alpha = 0.5) +
  theme_bw() +
  xlab('') + ylab('') +
  theme(axis.text=element_text(size=12, family='serif'),
        panel.background = element_rect(fill = 'lightblue')) +
  coord_map(projection='albers', lat0=35, lat1=55, xlim=c(-100, -60), ylim=c(35, 80))

```
**Figure 1**. *Location of sample plots used for analysis.  Samples are derived from a modern pollen dataset.*

To analyse the effects of non-analogue pollen on model skill we chose five pollen-based reconstruction methods from the literature and apply them to the modern pollen data from eastern North America.  The models are MAT, WA, WAPLS, BRT and randomForest.  Each of these models has been used previously in the pollen literature, and has strengths and weaknesses.  MAT (Overpeck, 1985) is commonly applied to reconstruct climate, for pollen, diatoms, chironomids and other taxa.  WA has a long history in the literature, and the recent development of monotone deshrinking has apparently improved performance further (Simpson, 2014?), but WA continues to suffer fro a basic assumption of unimodal taxon distributions along climate gradients.  

Results
=========================

When we examine the dissimilarity scores we can see that the distribution between minimum, maximum and overall dissimilarities can be understood numerically and spatially.  

Mean minimum dissimilarity for pollen samples in the modern pollen dataset ranges from `r min(min.diss)` to `r min(min.diss)`, with a 95% confidence interval from `r quantile(min.diss, 0.025)` to `r quantile(min.diss, 0.975)`.  The highest minimum analogue distance is for an unpublished site along the New York, Pennsylvannia border.  The site is notable for very high proportions of *Fraxinus pennsylvanica/americana* pollen, more than 5x higher than the next closest site.  The lowest minimum distance occurs across four sites, two from Thinn Lake, identified as potentially anomalous in Williams et al (2009), 


```{r Fig2MinDissim, fig.width=7, fig.height=6, fig.cap='Figure: North American site locations', echo=FALSE, message=FALSE, warning=FALSE}

dissim.scores <- data.frame(dissim = c(min.diss,
                                       as.vector(dists)[sample(length(dists), 100000)]),
                            class   = c(rep('Minimum', length(min.diss)),
                                        rep('All', 100000)))
              
ggplot(dissim.scores) + 
  geom_density(aes(x = dissim, linetype=class), alpha=0.5, size = 1.3, from = 0) +
  theme_bw() +
  xlab('Squared-chord Dissimilarity') +
  ylab('Kernel Density') +
  theme(text = element_text(size=24, family='serif'),
        strip.text = element_text(size=14, family='serif'),
        axis.text = element_text(size = 12, family='serif'),
        legend.position = 'none') +
  annotate("segment", x=0.48, 
           xend = 0.48, y = 0, yend = 0.1, size=2, color='black', alpha = 0.7) +
  scale_y_sqrt(limits = c(0, 5.5), expand=c(0,0)) +
  scale_x_continuous(expand=c(0,0))

```
**Figure 2**.  *Distribution of squared-chord dissimilarity values.  The 95% confidence interval for the minumum spanning distance in the North American Modern Pollen Database is 0.10 - 0.48.  We choose 0.48 (marked on the x axis) as our cut-off for 'non-analogue' distances in the database.*

```{r Fig3SiteMDS, fig.width=7, fig.height=6, fig.cap='Figure: North American site MDS scores', echo=FALSE, message=FALSE, warning=FALSE}

mds <- data.frame(pol.mds$points,
                  non.ana = factor(min.diss > 0.48))

non.ana <- ggplot(data = mds, aes(x = X1, y = X2)) +
  geom_point(data = mds[mds$non.ana == FALSE,], alpha=0.5, size = 2, color='black') +   
  geom_point(data = mds[mds$non.ana == TRUE,], alpha=0.7, size = 4, color='red') +
  theme_bw() +
  xlab('MDS Axis One') + ylab('MDS Axis Two') +
  theme(text = element_text(size=24, family='serif'))

non.ana.map <- ggplot(data = data.frame(map), aes(long, lat)) + 
  geom_polygon(aes(group=group), color='blue', alpha=0.2) +
  geom_point(data = data.frame(climate)[mds$non.ana == FALSE,], 
             aes(x = LONDD, y = LATDD), alpha=0.5) +
  geom_point(data = data.frame(climate)[mds$non.ana == TRUE,], 
             aes(x = LONDD, y = LATDD), color = 'red', size = 3) +
   theme_bw() +
  xlab('Longitude') + ylab('Latitude') +
  theme(axis.text=element_text(size=12, family='serif'),
        panel.background = element_rect(fill = 'lightblue')) +
  coord_map(projection='albers', lat0=35, lat1=55, xlim=c(-100, -60), ylim=c(35, 80))

grid.arrange(non.ana, non.ana.map, ncol=2)

```
**Figure 3**. *Location of non-analogue points in the modern pollen data, presented within a non-metric multidimensional scaling ordination (Panel 1) and geographically.  Interestingly, although a number of geographically isolated points exist in the dataset, the majority of non-analogue points exist within the most dense region of sampling, indicating that non-analogue sites are not ecologically stratified.*



Climate Variables
=========================

We used the `r nrow(climate)` records from the North American Modern Pollen Database and associated climate data to provide reconstructions of key climatic variables. The process of calculating the lambda/lambda ratio from RDA (ref) produces interesting relationships, perhaps impacted by the breadth of the available climate and pollen data across North America.

```{r echo = FALSE, message=FALSE, warning=FALSE}

#  Sakari, can you take a look at this and let me know what you think?  Am I doing this right, and what do you make of these results?  Are we simply using too large a dataset?

pol <- mod_pol[,7:ncol(mod_pol)]
clim <- climate[,4:ncol(climate)]

clim.ratio <- rep(NA, ncol(clim))

for(i in 1:ncol(clim)){
  test.struct <- rda(pol ~ clim[,i])
  clim.ratio[i] <- test.struct$CCA$tot.chi / test.struct$CA$tot.chi
}

names(clim.ratio) <- colnames(clim)
best.clim <- round(clim.ratio[order(clim.ratio, decreasing = TRUE)]*100, 0)

```

Variable | Percent Variability
-------- | -------------------
`r names(best.clim)[1]` | `r best.clim[1]`
`r names(best.clim)[2]` | `r best.clim[2]`
`r names(best.clim)[3]` | `r best.clim[3]`
`r names(best.clim)[4]` | `r best.clim[4]`
`r names(best.clim)[5]` | `r best.clim[5]`
`r names(best.clim)[6]` | `r best.clim[6]`
`r names(best.clim)[7]` | `r best.clim[7]`

Key variables based on the $latex \frac {\lambda_1} {\lambda_2} $ using pCCA appear to be climate variables relating to mean monthly percent summer sunshine (variables preceded by an 's').  These variables are likely to be variables that are combinations of temperature and preciptiation variables, however this value is not linear, nor is it continuous, meaning that .  While Telford & Birks (2009) have criticised sunshine reconstructions (Freshette et al., 2008) in the past (and perhaps with good reason), the strength of the sunshine signal in the climate data is worth noting, but the limitations of CCA may impact our ability to detect and accurately assess its importance.

Model outputs
=========================

Once the climate variables of interest were established we began by bootstrapping the models for these climate variables.  From the total pool of samples we generate a calibration dataset by sampling with replacement.  

```{r Fig4SampleLoss, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=6}

vals <- c(seq(0, 1, by=0.01))

na.samples <- data.frame(site = rep(climate[,1], ncol(mat.res$bias)),
                         size = as.vector(mat.res$sample_size),
                         distance = rep(seq(0, 1, by=0.01), each = nrow(mat.res$sample_size)))

na.samples <- na.samples[!is.na(na.samples$size),]

loss <- ggplot(na.samples, aes(x=distance, y=size)) + 
  geom_path(aes(group=site), alpha=0.01) +
  xlab('Analogue Distance') + ylab('Available Samples') +
  theme_bw() +
  theme(text = element_text(size=24, family='serif'),
        axis.text = element_text(size=14, family='serif')) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  geom_segment(aes(x=0.48, xend = 0.48, y = min(mat.res$sample_size, na.rm=TRUE), yend = nrow(climate)), size=1, color='red', linetype=2)
  
loc.loss <- data.frame(lat =  climate$LATDD,
                       long = climate$LONDD,
                       loss = 1 - mat.res$sample_size[,49] / nrow(climate))

loss.map <-  ggplot(data = data.frame(map), aes(long, lat)) + 
  geom_path(aes(group=group), color='black') +
  geom_point(data = loc.loss, 
             aes(x = long, y = lat, color = loss), alpha=0.5) +
   theme_bw() +
  theme(text = element_text(size=24, family='serif'),
        axis.text = element_text(size=14, family='serif')) +
  coord_map(projection='albers', lat0=30, lat1=50, xlim=c(-140, -60), ylim=c(26, 75)) +
  scale_color_continuous(low = '#F08080', high='#4169E1') +
  xlab('') + ylab('')

grid.arrange(loss, loss.map, ncol=2)

```
**Figure 4**. *As the analogue distance exclusion is increased the available data set size is reduced, although some points continue to maintain a large number of potential analogues, this is directly related to the minimum distance to the closest analogue.  Points that maintain a large pool of analogues are far from neighbours.*

```{r Fig5PredictedValues, message=FALSE, echo=FALSE, warning=FALSE}

init.color <- colorRampPalette(c('#F08080', '#68d64f', '#4169E1'))
clim.colors <- seq(0, 31, length.out = 300)
init.set <- init.color(300)[findInterval(x = climate[,10], vec=clim.colors)]

model.out <- data.frame(site   = rep(rep(climate[,1], ncol(wa.res$bias)),5),
                        preds  = c(as.vector(mat.res$mean_prediction),
                                   as.vector(wa.res$mean_prediction),
                                   as.vector(wapls.res$mean_prediction),
                                   as.vector(rfor.res$mean_prediction),
                                   as.vector(brt.res$mean_prediction)),
                        bias   = c(as.vector(mat.res$bias),
                                   as.vector(wa.res$bias),
                                   as.vector(wapls.res$bias),
                                   as.vector(rfor.res$bias),
                                   as.vector(brt.res$bias)),
                        variance = c(as.vector(mat.res$variance),
                                   as.vector(wa.res$variance),
                                   as.vector(wapls.res$variance),
                                   as.vector(rfor.res$variance),
                                   as.vector(brt.res$variance)),
                        dissim = rep(rep(vals, each=nrow(wa.res$bias)),5),
                        model  = rep(c('MAT', 'WA', 'WAPLS', 'rFor', 'BRT'), each = length(wa.res$bias)),
                        init = rep(rep(climate[,10], 101), 5))

#  This kills ~ 300,000 rows right now because of gaps in rfor.res and brt.res.
model.out <- model.out[!rowSums(is.na(model.out)) > 0,]

#  This is a temporary fix, 
sample.model <- function(x){
  #  How many samples exist for the model?
  set.len <- sum(model.out$model == x)
  if( set.len > 5000) set.len <- 5000
  
  set <- sample(which(model.out$model == x), set.len)
  start.end <- which(model.out$model == x & 
                       (model.out$dissim %in% c(0, 0.5, 1)))
  
  unique(c(set, start.end))
  
}

samp.set <- sapply(c('MAT', 'WA', 'WAPLS', 'rFor', 'BRT'), sample.model)

model.samp <- model.out[unlist(samp.set),]

#  This plot takes a really long time to plot.

ggplot(model.samp, aes(x = dissim, y = preds, group = site, color = init)) +
  geom_path(alpha = 0.1) +
  theme_bw() +
  facet_wrap(~model, nrow = 1) +
  scale_x_continuous(expand=c(0,0), breaks = c(0.5, 1)) +
  scale_y_continuous(expand=c(0,0), limits=c(0, 40)) +
  scale_color_continuous(low = 'blue', high='pink', trans='sqrt') +
  theme(legend.position = 'none',
        axis.title = element_text(family = 'serif', face = 'bold', size = 14),
        strip.text = element_text(family = 'serif', face = 'bold', size = 14),
        axis.text = element_text(family = 'serif', size = 10),
        strip.text = element_text(family = 'serif', face = 'bold', size = 12)) +
  annotate("segment", x=0.48, 
           xend = 0.48, y = 0, yend = 100, size=1, linetype='dashed') +
  xlab('Calibration Dissimilarity') +
  ylab('Predicted Value')

```
**Figure 5**. *Change in site predictions over time.  Of great interest is the difference between model outputs as the exclusion zone increases.  MAT can never predict beyond the extent of the data samples in the pool, and so as the exclusion zone increases the predictions move toward the dataset mean.  WAPLS uses an initial transformation and then regresses against it to predict new values, so as the exclusion zone increases we see an increase in the spread of the predicted values, with much greater variability and a loss of accuracy.*

```{r Fig6BiasValues, message=FALSE, echo=FALSE, warning=FALSE}

ggplot(model.samp, aes(x = dissim, y = sqrt(bias), group = site, color = init)) +
  geom_path(alpha = 0.1) +
  theme_bw() +
  facet_wrap(~model, nrow = 1) +
  scale_x_continuous(expand=c(0,0), breaks = c(0.5, 1)) +
  scale_y_sqrt(expand=c(0,0)) +
  scale_color_continuous(low = 'blue', high='pink', trans='sqrt') +
  theme(legend.position = 'none',
        axis.title = element_text(family = 'serif', face = 'bold', size = 14),
        strip.text = element_text(family = 'serif', face = 'bold', size = 14),
        axis.text = element_text(family = 'serif', size = 10),
        strip.text = element_text(family = 'serif', face = 'bold', size = 12)) +
  xlab('Calibration Dissimilarity') +
  ylab('Sample Bias (RMSEP)')

```
**Figure 6**. *Change in model bias with increasing analogue distance.*

```{r Fig7VarianceValues, message=FALSE, echo=FALSE, warning=FALSE}

ggplot(model.samp, aes(x = dissim, y = sqrt(variance), group = site, color = init)) +
  geom_path(alpha = 0.1) +
  theme_bw() +
  facet_wrap(~model, nrow = 1) +
  scale_x_continuous(expand=c(0,0), breaks = c(0.5, 1)) +
  scale_y_sqrt(expand=c(0,0)) +
  scale_color_continuous(low = 'blue', high='pink', trans='sqrt') +
  theme(legend.position = 'none',
        axis.title = element_text(family = 'serif', face = 'bold', size = 14),
        strip.text = element_text(family = 'serif', face = 'bold', size = 14),
        axis.text = element_text(family = 'serif', size = 10),
        strip.text = element_text(family = 'serif', face = 'bold', size = 12)) +
  xlab('Calibration Dissimilarity') +
  ylab('Bootstrap Variance')

```
**Figure 7**. *Change in bootstrap variance with increasing analogue distance.*

Discussion
=========================
What does this tell us about predicting climate in non-analogue space?

References
=========================
Bartlein, P. J., Harrison, S. P., Brewer, S., Connor, S., Davis, B. A. S., Gajewski, K., ... & Wu, H. (2011). Pollen-based continental climate reconstructions at 6 and 21 ka: a global synthesis. Climate Dynamics, 37(3-4), 775-802.

Brewer, S., Guiot, J., Sánchez-Goñi, M. F., & Klotz, S. (2008). The climate in Europe during the Eemian: a multi-method approach using pollen data. Quaternary Science Reviews, 27(25), 2303-2315.